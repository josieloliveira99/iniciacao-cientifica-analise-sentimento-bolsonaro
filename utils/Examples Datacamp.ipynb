{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())\n",
    "\n",
    "# Amazing work! You now know how to compute the number of words in a given piece of text. Also, notice that the average length of a talk is close to 2000 words. You can use the word_count feature to compute its correlation with other variables such as number of views, number of comments, etc. and derive extremely interesting insights about TED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets[\"content\"].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets[\"char_count\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns numner of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eu', 'sou', 'o', 'Bolsonaro', 'e', 'não', 'sei', 'o', 'que', 'estou', 'escrevendo', ',', 'agora', '.', '#', 'naosei']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "gettysburg = \"eu sou o Bolsonaro e não sei o que estou escrevendo, agora. #naosei\"\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bolsonaro,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu não saber o que estar escrever , agora . # naosei\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "# doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu não sei que estou escrevendo agora naosei\n"
     ]
    }
   ],
   "source": [
    "# Load model and create Doc object\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.text for token in doc]\n",
    "\n",
    "stopwords = [\"a\",\"e\",\"i\",\"o\",\"u\"]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eu', 'PRON'), ('sou', 'VERB'), ('o', 'DET'), ('Bolsonaro', 'PROPN'), ('e', 'CCONJ'), ('não', 'ADV'), ('sei', 'VERB'), ('muito', 'ADV'), ('bem', 'ADV'), ('o', 'X'), ('que', 'PRON'), ('estou', 'VERB'), ('escrevendo', 'VERB'), (',', 'PUNCT'), ('agora', 'ADV'), ('.', 'PUNCT'), ('#', 'PROPN'), ('naosei', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "example = \"eu sou o Bolsonaro e não sei muito bem o que estou escrevendo, agora. #naosei\"\n",
    "doc = nlp(example)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'VERB', 'DET', 'PROPN', 'CCONJ', 'ADV', 'VERB', 'ADV', 'ADV', 'X', 'PRON', 'VERB', 'VERB', 'PUNCT', 'ADV', 'PUNCT', 'PROPN', 'NOUN']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    print(pos)\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count(\"PROPN\")\n",
    "\n",
    "print(proper_nouns(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, you have been given a dataframe headlines that contains news headlines that are either fake or real. Your task is to generate two new features num_propn and num_noun that represent the number of proper nouns and other nouns contained in the title feature of headlines.\n",
    "\n",
    "# Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the num_propn and num_noun features in fake news detectors will improve its performance.\n",
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))\n",
    "\n",
    "# Excellent work! You now know to construct features using POS tags information. Notice how the mean number of proper nouns is considerably higher for fake news than it is for real news. The opposite seems to be true in the case of other nouns. This fact can be put to great use in desgning fake news detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "# Sundar Pichai ORG\n",
    "# Google ORG\n",
    "# Mountain View GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, you have been given an excerpt from a news article published in TechCrunch. Your task is to write a function find_people that identifies the names of people that have been mentioned in a particular piece of text. You will then use find_people to identify the people of interest in the article.\n",
    "\n",
    "# The article is available as the string tc and has been printed to the console. The required spacy model has also been already loaded as nlp.\n",
    "\n",
    "def find_persons(text,model=nlp):\n",
    "  # Create Doc object\n",
    "  doc = model(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing dimensionality and preprocessing\n",
    "# In this exercise, you have been provided with a lem_corpus which contains the pre-processed versions of the movie taglines from the previous exercise. In other words, the taglines have been lowercased and lemmatized, and stopwords have been removed.\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
    "\n",
    "# Print the shape of bow_lem_matrix\n",
    "print(bow_lem_matrix.shape)\n",
    "\n",
    "# Good job! Notice how the number of features have reduced significantly from around 6600 to around 5223 for pre-processed movie taglines. The reduced number of dimensions on account of text preprocessing usually leads to better performance when conducting machine learning and it is a good idea to consider it. However, as mentioned in a previous lesson, the final decision always depends on the nature of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the lesson video, we had seen that CountVectorizer doesn't necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary.\n",
    "# We will use the same three sentences on lions from the video. The sentences are available in a list named corpus and has already been printed to the console.\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)\n",
    "\n",
    "#    an  decade  endangered  have  is  ...  lion  lions  of  species  the\n",
    "# 0   0       0           0     0   1  ...     1      0   1        0    3\n",
    "# 1   0       1           0     1   0  ...     0      1   1        0    0\n",
    "# 2   1       0           1     0   1  ...     1      0   0        1    1\n",
    "\n",
    "# Observe that the column names refer to the token whose frequency is being recorded. Therefore, since the first column name is an, the first feature represents the number of times the word 'an' occurs in a particular sentence. get_feature_names() essentially gives us a list which represents the mapping of the feature indices to the feature name in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW vectors for movie reviews\n",
    "# In this exercise, you have been given two pandas Series, X_train and X_test, which consist of movie reviews. They represent the training and the test review data respectively. Your task is to preprocess the reviews and generate BoW vectors for these two sets using CountVectorizer.\n",
    "# Once we have generated the BoW vector matrices X_train_bow and X_test_bow, we will be in a very good position to apply a machine learning model to it and conduct sentiment analysis.\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)\n",
    "\n",
    "#  In this exercise, you have set the lowercase argument to True. However, note that this is the default value of lowercase and passing it explicitly is not necessary. Also, note that both X_train_bow and X_test_bow have 8158 features. There were words present in X_test that were not in X_train. CountVectorizer chose to ignore them in order to ensure that the dimensions of both sets remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the sentiment of a movie review\n",
    "# In the previous exercise, you generated the bag-of-words representations for the training and test movie review data. In this exercise, we will use this model to train a Naive Bayes classifier that can detect the sentiment of a movie review and compute its accuracy. Note that since this is a binary classification problem, the model is only capable of classifying a review as either positive (1) or negative (0). It is incapable of detecting neutral reviews.\n",
    "# \n",
    "# In case you don't recall, the training and test BoW vectors are available as X_train_bow and X_test_bow respectively. The corresponding labels are available as y_train and y_test respectively. Also, for you reference, the original movie review dataset is available as df.\n",
    "\n",
    "\n",
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram models for movie tag lines\n",
    "# In this exercise, we have been provided with a corpus of more than 9000 movie tag lines. Our job is to generate n-gram models up to n equal to 1, n equal to 2 and n equal to 3 for this data and discover the number of features for each model.\n",
    "# \n",
    "# We will then compare the number of features generated for each model.\n",
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1,3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))\n",
    "\n",
    "# ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively\n",
    "\n",
    "# Notice that ng2 has over 37,000 features whereas ng3 has over 76,000 features. This is much greater than the 6,000 dimensions obtained for ng1. As the n-gram range increases, so does the number of features, leading to increased computational costs and a problem known as the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher order n-grams for sentiment analysis\n",
    "# Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task.\n",
    "# \n",
    "# The n-gram training reviews are available as X_train_ng. The corresponding test reviews are available as X_test_ng. Finally, use y_train and y_test to access the training and test sentiment classes respectively.\n",
    "\n",
    "# Define an instance of MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing performance of n-gram models\n",
    "# You now know how to conduct sentiment analysis by converting text into various n-gram representations and feeding them to a classifier. In this exercise, we will conduct sentiment analysis for the same movie reviews from before using two n-gram models: unigrams and n-grams upto n equal to 3.\n",
    "# \n",
    "# We will then compare the performance using three criteria: accuracy of the model on the test set, time taken to execute the program and the number of features created when generating the n-gram representation.\n",
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer()\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))\n",
    "# The program took 0.203 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))\n",
    "\n",
    "# The program took 2.839 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bolsonaro', 'bolsonaro um', 'bosta', 'cara', 'cara maluco', 'de', 'de bosta', 'maluco', 'pau', 'pau de', 'um', 'um cara', 'um pau']\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "train_X = [\"o bolsonaro é um cara maluco.\",\"o bolsonaro é um pau de bosta.\"]\n",
    "# ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(1, 2))\n",
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "counts = ngram_vectorizer.fit_transform(train_X)\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(ngram_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "# (500, 29158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity matrix of a corpus\n",
    "# In this exercise, you have been given a corpus, which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
    "# \n",
    "# Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector.\n",
    "\n",
    "# corpus:\n",
    "#  ['The sun is the largest celestial body in the solar system', 'The solar system consists of the sun and eight revolving planets', 'Ra was the Egyptian Sun God', 'The Pyramids were the pinnacle of Egyptian architecture', 'The quick brown fox jumps over the lazy dog']\n",
    "\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix,tfidf_matrix)\n",
    "print(cosine_sim)\n",
    "\n",
    "# <script.py> output:\n",
    "#     [[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
    "#      [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
    "#      [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
    "#      [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
    "#      [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))\n",
    "\n",
    "# When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations\n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O 1.0\n",
      "O bolsonaro 0.26903608\n",
      "O não 0.055895984\n",
      "O está -0.18391667\n",
      "O muito 0.076491915\n",
      "O feliz 0.05900094\n",
      "O e 0.15554796\n",
      "O carlos 0.016374692\n",
      "O está -0.15701915\n",
      "O bem -0.023924652\n",
      "O pouco 0.080980614\n",
      "O triste -0.008622016\n",
      "bolsonaro O 0.26903608\n",
      "bolsonaro bolsonaro 1.0\n",
      "bolsonaro não 0.34484246\n",
      "bolsonaro está 0.03245093\n",
      "bolsonaro muito -0.10902406\n",
      "bolsonaro feliz 0.005839282\n",
      "bolsonaro e 0.016258106\n",
      "bolsonaro carlos 0.21898137\n",
      "bolsonaro está 0.032048874\n",
      "bolsonaro bem 0.02846189\n",
      "bolsonaro pouco -0.012788937\n",
      "bolsonaro triste -0.10803997\n",
      "não O 0.055895984\n",
      "não bolsonaro 0.34484246\n",
      "não não 1.0\n",
      "não está 0.17289019\n",
      "não muito 0.020479094\n",
      "não feliz 0.0032494238\n",
      "não e 0.03251043\n",
      "não carlos 0.18118812\n",
      "não está 0.18776192\n",
      "não bem 0.124404795\n",
      "não pouco 0.14052448\n",
      "não triste -0.103170864\n",
      "está O -0.18391667\n",
      "está bolsonaro 0.03245093\n",
      "está não 0.17289019\n",
      "está está 1.0\n",
      "está muito 0.07637086\n",
      "está feliz -0.044791345\n",
      "está e 0.11929379\n",
      "está carlos -0.10268203\n",
      "está está 1.0\n",
      "está bem 0.28941473\n",
      "está pouco -0.03299762\n",
      "está triste 0.14422254\n",
      "muito O 0.076491915\n",
      "muito bolsonaro -0.10902406\n",
      "muito não 0.020479094\n",
      "muito está 0.07637086\n",
      "muito muito 1.0\n",
      "muito feliz 0.21628664\n",
      "muito e 0.07633588\n",
      "muito carlos 0.021843381\n",
      "muito está 0.05863479\n",
      "muito bem 0.7092932\n",
      "muito pouco 0.65585655\n",
      "muito triste 0.11060678\n",
      "feliz O 0.05900094\n",
      "feliz bolsonaro 0.005839282\n",
      "feliz não 0.0032494238\n",
      "feliz está -0.044791345\n",
      "feliz muito 0.21628664\n",
      "feliz feliz 1.0\n",
      "feliz e 0.07682993\n",
      "feliz carlos 0.14989164\n",
      "feliz está -0.05645351\n",
      "feliz bem 0.18249042\n",
      "feliz pouco 0.24960813\n",
      "feliz triste 0.29628265\n",
      "e O 0.15554796\n",
      "e bolsonaro 0.016258106\n",
      "e não 0.03251043\n",
      "e está 0.11929379\n",
      "e muito 0.07633588\n",
      "e feliz 0.07682993\n",
      "e e 1.0\n",
      "e carlos -0.02957385\n",
      "e está 0.10183838\n",
      "e bem 0.048411746\n",
      "e pouco 0.12059456\n",
      "e triste -0.11081912\n",
      "carlos O 0.016374692\n",
      "carlos bolsonaro 0.21898137\n",
      "carlos não 0.18118812\n",
      "carlos está -0.10268203\n",
      "carlos muito 0.021843381\n",
      "carlos feliz 0.14989164\n",
      "carlos e -0.02957385\n",
      "carlos carlos 1.0\n",
      "carlos está 0.028349755\n",
      "carlos bem 0.055803504\n",
      "carlos pouco 0.21628387\n",
      "carlos triste -0.048135094\n",
      "está O -0.15701915\n",
      "está bolsonaro 0.032048874\n",
      "está não 0.18776192\n",
      "está está 1.0\n",
      "está muito 0.05863479\n",
      "está feliz -0.05645351\n",
      "está e 0.10183838\n",
      "está carlos 0.028349755\n",
      "está está 1.0\n",
      "está bem 0.29728472\n",
      "está pouco 0.029453551\n",
      "está triste 0.13866927\n",
      "bem O -0.023924652\n",
      "bem bolsonaro 0.02846189\n",
      "bem não 0.124404795\n",
      "bem está 0.28941473\n",
      "bem muito 0.7092932\n",
      "bem feliz 0.18249042\n",
      "bem e 0.048411746\n",
      "bem carlos 0.055803504\n",
      "bem está 0.29728472\n",
      "bem bem 1.0\n",
      "bem pouco 0.5355247\n",
      "bem triste 0.14583918\n",
      "pouco O 0.080980614\n",
      "pouco bolsonaro -0.012788937\n",
      "pouco não 0.14052448\n",
      "pouco está -0.03299762\n",
      "pouco muito 0.65585655\n",
      "pouco feliz 0.24960813\n",
      "pouco e 0.12059456\n",
      "pouco carlos 0.21628387\n",
      "pouco está 0.029453551\n",
      "pouco bem 0.5355247\n",
      "pouco pouco 1.0\n",
      "pouco triste 0.19301103\n",
      "triste O -0.008622016\n",
      "triste bolsonaro -0.10803997\n",
      "triste não -0.103170864\n",
      "triste está 0.14422254\n",
      "triste muito 0.11060678\n",
      "triste feliz 0.29628265\n",
      "triste e -0.11081912\n",
      "triste carlos -0.048135094\n",
      "triste está 0.13866927\n",
      "triste bem 0.14583918\n",
      "triste pouco 0.19301103\n",
      "triste triste 1.0\n"
     ]
    }
   ],
   "source": [
    "sent = \"O bolsonaro não está muito feliz e carlos está bem pouco triste\"\n",
    "\n",
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "  for token2 in doc:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog index and consequently, determine the relative difficulty of reading these publications.\n",
    "# \n",
    "# The excerpts are available as the following strings:\n",
    "# \n",
    "# forbes- An excerpt from an article from Forbes magazine on the Chinese social credit score system.\n",
    "# harvard_law- An excerpt from a book review published in Harvard Law Review.\n",
    "# r_digest- An excerpt from a Reader's Digest article on flight turbulence.\n",
    "# time_kids - An excerpt from an article on the ill effects of salt consumption published in TIME for Kids.\n",
    "\n",
    "\n",
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "  \n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)\n",
    "\n",
    "# Notice that the Harvard Law Review excerpt has the highest Gunning fog index; indicating that it can be comprehended only by readers who have graduated college. On the other hand, the Time for Kids article, intended for children, has a much lower fog index and can be comprehended by 5th grade students."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
